{"cells":[{"cell_type":"markdown","source":["# Step 3: Scoring Pipeline\n\nWe assume you have already created a model to use in this scoring script. You can create a model using the `.\\notebooks\\2_Training_Pipeline` notebook.\n\nThe scoring pipeline consists of two steps:\n\n  1. Transform the raw data into a scoring data set\n  2. Score the scoring data set.\n  \nWe assume that new data is arriving into the data store as it's generated. A scoring workflow would poll the data store for new data on whatever schedule is convenient (realtime, hourly, daily...), transform and manipulate the new data just as was done for the training step, then predict the label for the new observations.\n\nWe can run the feature engineering notebook (`./notebooks/2a_feature_engineering`) with the correct parameters to simulate the polling process. Since our data was all ingested, and we are not simulating the collection of new data, we choose observations after the training data set end date of \"2015-10-30\". We transform the data into observation format the model expects. A note here, in a true production setting, it might be more convenient to split the (`./notebooks/2a_feature_engineering`) notebook into a featurizing notebook and a labeller notebook that calculates labels separately, and joins them to the features. Then the scoring process would only need the features, and labels could be collected later and compared to predictions for post processing to monitor the model accuracy. \n\nSecond, we run the scoring notebook (`./notebooks/3a_model_scoring`) to read the scoring data set, generate predictions for each of those observations using the specified `model` and store the results in the `results_data` dataset. \n\nSince the scoring data set is only used between these two notebooks, we remove the scoring table after scoring the data."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\n\n# The scoring uses the same feature engineering script used to train the model\nscoring_table = 'HPscoring_input'\nresults_table = 'HPresults_output'\nmodel = 'RandomForest' # Use 'DecisionTree' or 'RandomForest'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["Databricks parameters to customize the runs."],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.removeAll()\ndbutils.widgets.text(\"results_data\", results_table)\n\ndbutils.widgets.text(\"model\", model)\n\ndbutils.widgets.text(\"start_date\", '2015-11-15')\n\ndbutils.widgets.text(\"to_date\", '2016-04-30')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["## Feature Engineering\n\nCreate a scoring data set using the parameters `start_date` and `to_date` to select the time period for scoring. Store those results in the `scoring_table` specified."],"metadata":{}},{"cell_type":"code","source":["dbutils.notebook.run(\"2a_feature_engineering\", 600, {\"features_table\": scoring_table, \n                                                     \"start_date\": dbutils.widgets.get(\"start_date\"), \n                                                     \"to_date\": dbutils.widgets.get(\"to_date\")})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## Scoring observations\n\nUsing the `model` specified, predict the probability of component failures for the observations in the `scoring_table`. Store the resulting probabilities in the `results_data` set, which will be available in the Databricks data store for later post processing."],"metadata":{}},{"cell_type":"code","source":["dbutils.notebook.run(\"3a_model_scoring\", 600, {\"scoring_data\": scoring_table, \n                                               \"results_data\": dbutils.widgets.get(\"results_data\"), \n                                               \"model\": dbutils.widgets.get(\"model\")})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## Cleanup temporary data\n\nSince we only need the `scoring_table` data to pass observations from the featurizer to the scoring notebooks, we can safly remove the table."],"metadata":{}},{"cell_type":"code","source":["# Since we created the scoring data set, we should remove it to keep things clean.\nspark = SparkSession.builder.getOrCreate()\ntelemetry = spark.sql(\"DROP TABLE \" + scoring_table)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["# Conclusion"],"metadata":{}}],"metadata":{"name":"Scoring Workflow","notebookId":1402090728699124},"nbformat":4,"nbformat_minor":0}
